{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.segmentation.cell_segmentation.utils import ViTCellViT, ViTCellViTDeit\n",
    "from models.segmentation.cell_segmentation.cellvit import Deconv2DBlock, Conv2DBlock\n",
    "from functools import partial\n",
    "\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import List, Literal, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ViTStarDist(nn.Module):\n",
    "    \"\"\"CellViT with SAM backbone settings\n",
    "\n",
    "    Skip connections are shared between branches\n",
    "\n",
    "    Args:\n",
    "        model_path (Union[Path, str]): Path to pretrained SAM model\n",
    "        num_nuclei_classes (int): Number of nuclei classes (including background)\n",
    "        num_tissue_classes (int): Number of tissue classes\n",
    "        vit_structure (Literal[\"SAM-B\", \"SAM-L\", \"SAM-H\"]): SAM model type\n",
    "        drop_rate (float, optional): Dropout in MLP. Defaults to 0.\n",
    "        regression_loss (bool, optional): Use regressive loss for predicting vector components.\n",
    "            Adds two additional channels to the binary and hv decoder. Defaults to False.\n",
    "\n",
    "    Raises:\n",
    "        NotImplementedError: Unknown SAM configuration\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        num_nuclei_classes,\n",
    "        num_tissue_classes,\n",
    "        vit_structure,\n",
    "        drop_rate,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if vit_structure == \"SAM-B\":\n",
    "            self.init_vit_b()\n",
    "        elif vit_structure == \"SAM-L\":\n",
    "            self.init_vit_l()\n",
    "        elif vit_structure == \"SAM-H\":\n",
    "            self.init_vit_h()\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unknown ViT-SAM backbone structure\")\n",
    "\n",
    "        self.drop_rate = drop_rate\n",
    "        self.patch_size = 16\n",
    "        self.input_channels = 3\n",
    "        self.mlp_ratio = 4\n",
    "        self.qkv_bias = True\n",
    "        self.num_nuclei_classes = num_nuclei_classes\n",
    "        self.model_path = model_path\n",
    "        self.prompt_embed_dim = 256\n",
    "        self.nrays = 32\n",
    "        \n",
    "        \n",
    "        if self.embed_dim < 512:\n",
    "            self.skip_dim_11 = 256\n",
    "            self.skip_dim_12 = 128\n",
    "            self.bottleneck_dim = 312\n",
    "        else:\n",
    "            self.skip_dim_11 = 512\n",
    "            self.skip_dim_12 = 256\n",
    "            self.bottleneck_dim = 512\n",
    "        \n",
    "        self.encoder = ViTCellViTDeit(\n",
    "            extract_layers=self.extract_layers,\n",
    "            depth=self.depth,\n",
    "            embed_dim=self.embed_dim,\n",
    "            mlp_ratio=4,\n",
    "            norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n",
    "            num_heads=self.num_heads,\n",
    "            qkv_bias=True,\n",
    "            use_rel_pos=True,\n",
    "            global_attn_indexes=self.encoder_global_attn_indexes,\n",
    "            window_size=14,\n",
    "            out_chans=self.prompt_embed_dim,\n",
    "        )\n",
    "\n",
    "        self.decoder0 = nn.Sequential(\n",
    "            Conv2DBlock(3, 32, 3, dropout=self.drop_rate),\n",
    "            Conv2DBlock(32, 64, 3, dropout=self.drop_rate),\n",
    "        )  # skip connection after positional encoding, shape should be H, W, 64\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            Deconv2DBlock(self.embed_dim, self.skip_dim_11, dropout=self.drop_rate),\n",
    "            Deconv2DBlock(self.skip_dim_11, self.skip_dim_12, dropout=self.drop_rate),\n",
    "            Deconv2DBlock(self.skip_dim_12, 128, dropout=self.drop_rate),\n",
    "        )  # skip connection 1\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            Deconv2DBlock(self.embed_dim, self.skip_dim_11, dropout=self.drop_rate),\n",
    "            Deconv2DBlock(self.skip_dim_11, 256, dropout=self.drop_rate),\n",
    "        )  # skip connection 2\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            Deconv2DBlock(self.embed_dim, self.bottleneck_dim, dropout=self.drop_rate)\n",
    "        )\n",
    "        self.branches_output = {\n",
    "            \"stardist\": self.nrays,\n",
    "            \"dist\": 1,\n",
    "            \"nuclei_type_maps\": self.num_nuclei_classes,\n",
    "        }\n",
    "        self.stardist_decoder = self.create_upsampling_branch(\n",
    "            self.branches_output[\"stardist\"]\n",
    "        )\n",
    "        self.dist_decoder = self.create_upsampling_branch(\n",
    "            self.branches_output[\"dist\"]\n",
    "        )\n",
    "        self.nuclei_type_maps_decoder = self.create_upsampling_branch(\n",
    "            self.num_nuclei_classes\n",
    "        )\n",
    "        self.classifier_head = (\n",
    "            nn.Linear(self.prompt_embed_dim, num_tissue_classes)\n",
    "            if num_tissue_classes > 0\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def load_pretrained_encoder(self, model_path):\n",
    "        \"\"\"Load pretrained SAM encoder from provided path\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to SAM model\n",
    "        \"\"\"\n",
    "        state_dict = torch.load(str(model_path), map_location=\"cpu\")\n",
    "        image_encoder = self.encoder\n",
    "        msg = image_encoder.load_state_dict(state_dict, strict=False)\n",
    "        print(f\"Loading checkpoint: {msg}\")\n",
    "        self.encoder = image_encoder\n",
    "\n",
    "    def forward(self, x: torch.Tensor, retrieve_tokens: bool = False):\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Images in BCHW style\n",
    "            retrieve_tokens (bool, optional): If tokens of ViT should be returned as well. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            dict: Output for all branches:\n",
    "                * tissue_types: Raw tissue type prediction. Shape: (batch_size, num_tissue_classes)\n",
    "                * nuclei_binary_map: Raw binary cell segmentation predictions. Shape: (batch_size, 2, H, W)\n",
    "                * hv_map: Binary HV Map predictions. Shape: (batch_size, 2, H, W)\n",
    "                * nuclei_type_map: Raw binary nuclei type preditcions. Shape: (batch_size, num_nuclei_classes, H, W)\n",
    "                * (optinal) tokens\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            x.shape[-2] % self.patch_size == 0\n",
    "        ), \"Img must have a shape of that is divisble by patch_soze (token_size)\"\n",
    "        assert (\n",
    "            x.shape[-1] % self.patch_size == 0\n",
    "        ), \"Img must have a shape of that is divisble by patch_soze (token_size)\"\n",
    "\n",
    "        out_dict = {}\n",
    "\n",
    "        classifier_logits, _, z = self.encoder(x)\n",
    "        out_dict[\"tissue_types\"] = self.classifier_head(classifier_logits)\n",
    "\n",
    "        z0, z1, z2, z3, z4 = x, *z\n",
    "\n",
    "        # performing reshape for the convolutional layers and upsampling (restore spatial dimension)\n",
    "        z4 = z4.permute(0, 3, 1, 2)\n",
    "        z3 = z3.permute(0, 3, 1, 2)\n",
    "        z2 = z2.permute(0, 3, 1, 2)\n",
    "        z1 = z1.permute(0, 3, 1, 2)\n",
    "\n",
    "        out_dict[\"stardist\"] = self._forward_upsample(\n",
    "            z0, z1, z2, z3, z4, self.stardist_decoder\n",
    "        )\n",
    "        out_dict[\"dist\"] = self._forward_upsample(\n",
    "            z0, z1, z2, z3, z4, self.dist_decoder\n",
    "        )\n",
    "        out_dict[\"nuclei_type_map\"] = self._forward_upsample(\n",
    "            z0, z1, z2, z3, z4, self.nuclei_type_maps_decoder\n",
    "        )\n",
    "\n",
    "        if retrieve_tokens:\n",
    "            out_dict[\"tokens\"] = z4\n",
    "\n",
    "        return out_dict\n",
    "\n",
    "    def _forward_upsample(\n",
    "        self,\n",
    "        z0: torch.Tensor,\n",
    "        z1: torch.Tensor,\n",
    "        z2: torch.Tensor,\n",
    "        z3: torch.Tensor,\n",
    "        z4: torch.Tensor,\n",
    "        branch_decoder: nn.Sequential,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward upsample branch\n",
    "\n",
    "        Args:\n",
    "            z0 (torch.Tensor): Highest skip\n",
    "            z1 (torch.Tensor): 1. Skip\n",
    "            z2 (torch.Tensor): 2. Skip\n",
    "            z3 (torch.Tensor): 3. Skip\n",
    "            z4 (torch.Tensor): Bottleneck\n",
    "            branch_decoder (nn.Sequential): Branch decoder network\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Branch Output\n",
    "        \"\"\"\n",
    "        b4 = branch_decoder.bottleneck_upsampler(z4)\n",
    "        b3 = branch_decoder.decoder3_skip(z3)\n",
    "        b3 = branch_decoder.decoder3_upsampler(torch.cat([b3, b4], dim=1))\n",
    "        b2 = branch_decoder.decoder2_skip(z2)\n",
    "        b2 = branch_decoder.decoder2_upsampler(torch.cat([b2, b3], dim=1))\n",
    "        b1 = branch_decoder.decoder1_skip(z1)\n",
    "        b1 = branch_decoder.decoder1_upsampler(torch.cat([b1, b2], dim=1))\n",
    "        b0 = branch_decoder.decoder0_skip(z0)\n",
    "        branch_output = branch_decoder.decoder0_header(torch.cat([b0, b1], dim=1))\n",
    "\n",
    "        return branch_output\n",
    "\n",
    "    def create_upsampling_branch(self, num_classes: int) -> nn.Module:\n",
    "        \"\"\"Create Upsampling branch\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Number of output classes\n",
    "\n",
    "        Returns:\n",
    "            nn.Module: Upsampling path\n",
    "        \"\"\"\n",
    "        # Skip connections\n",
    "        decoder0_skip = nn.Sequential(\n",
    "            Conv2DBlock(3, 32, 3, self.drop_rate),\n",
    "            Conv2DBlock(32, 64, 3, self.drop_rate),\n",
    "        )  # skip connection after positional encoding, shape should be H, W, 64\n",
    "        decoder1_skip = nn.Sequential(\n",
    "            Deconv2DBlock(self.embed_dim, self.skip_dim_11, dropout=self.drop_rate),\n",
    "            Deconv2DBlock(self.skip_dim_11, self.skip_dim_12, dropout=self.drop_rate),\n",
    "            Deconv2DBlock(self.skip_dim_12, 128, dropout=self.drop_rate),\n",
    "        )  # skip connection 1\n",
    "        decoder2_skip = nn.Sequential(\n",
    "            Deconv2DBlock(self.embed_dim, self.skip_dim_11, dropout=self.drop_rate),\n",
    "            Deconv2DBlock(self.skip_dim_11, 256, dropout=self.drop_rate),\n",
    "        )  # skip connection 2\n",
    "        decoder3_skip = nn.Sequential(\n",
    "            Deconv2DBlock(self.embed_dim, self.bottleneck_dim, dropout=self.drop_rate)\n",
    "        )  # skip connection 3\n",
    "\n",
    "        # Upsampling\n",
    "        bottleneck_upsampler = nn.ConvTranspose2d(\n",
    "            in_channels=self.embed_dim,\n",
    "            out_channels=self.bottleneck_dim,\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "            padding=0,\n",
    "            output_padding=0,\n",
    "        )\n",
    "        decoder3_upsampler = nn.Sequential(\n",
    "            Conv2DBlock(\n",
    "                self.bottleneck_dim * 2, self.bottleneck_dim, dropout=self.drop_rate\n",
    "            ),\n",
    "            Conv2DBlock(\n",
    "                self.bottleneck_dim, self.bottleneck_dim, dropout=self.drop_rate\n",
    "            ),\n",
    "            Conv2DBlock(\n",
    "                self.bottleneck_dim, self.bottleneck_dim, dropout=self.drop_rate\n",
    "            ),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=self.bottleneck_dim,\n",
    "                out_channels=256,\n",
    "                kernel_size=2,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                output_padding=0,\n",
    "            ),\n",
    "        )\n",
    "        decoder2_upsampler = nn.Sequential(\n",
    "            Conv2DBlock(256 * 2, 256, dropout=self.drop_rate),\n",
    "            Conv2DBlock(256, 256, dropout=self.drop_rate),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=256,\n",
    "                out_channels=128,\n",
    "                kernel_size=2,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                output_padding=0,\n",
    "            ),\n",
    "        )\n",
    "        decoder1_upsampler = nn.Sequential(\n",
    "            Conv2DBlock(128 * 2, 128, dropout=self.drop_rate),\n",
    "            Conv2DBlock(128, 128, dropout=self.drop_rate),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=128,\n",
    "                out_channels=64,\n",
    "                kernel_size=2,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                output_padding=0,\n",
    "            ),\n",
    "        )\n",
    "        decoder0_header = nn.Sequential(\n",
    "            Conv2DBlock(64 * 2, 64, dropout=self.drop_rate),\n",
    "            Conv2DBlock(64, 64, dropout=self.drop_rate),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=num_classes,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        decoder = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"decoder0_skip\", decoder0_skip),\n",
    "                    (\"decoder1_skip\", decoder1_skip),\n",
    "                    (\"decoder2_skip\", decoder2_skip),\n",
    "                    (\"decoder3_skip\", decoder3_skip),\n",
    "                    (\"bottleneck_upsampler\", bottleneck_upsampler),\n",
    "                    (\"decoder3_upsampler\", decoder3_upsampler),\n",
    "                    (\"decoder2_upsampler\", decoder2_upsampler),\n",
    "                    (\"decoder1_upsampler\", decoder1_upsampler),\n",
    "                    (\"decoder0_header\", decoder0_header),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return decoder\n",
    "\n",
    "    def init_vit_b(self):\n",
    "        self.embed_dim = 768\n",
    "        self.depth = 12\n",
    "        self.num_heads = 12\n",
    "        self.encoder_global_attn_indexes = [2, 5, 8, 11]\n",
    "        self.extract_layers = [3, 6, 9, 12]\n",
    "\n",
    "    def init_vit_l(self):\n",
    "        self.embed_dim = 1024\n",
    "        self.depth = 24\n",
    "        self.num_heads = 16\n",
    "        self.encoder_global_attn_indexes = [5, 11, 17, 23]\n",
    "        self.extract_layers = [6, 12, 18, 24]\n",
    "\n",
    "    def init_vit_h(self):\n",
    "        self.embed_dim = 1280\n",
    "        self.depth = 32\n",
    "        self.num_heads = 16\n",
    "        self.encoder_global_attn_indexes = [7, 15, 23, 31]\n",
    "        self.extract_layers = [8, 16, 24, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CellViTStarDist' from 'models.segmentation.cell_segmentation.cellvit' (/homes/fhoerst/histo-projects/CellViT/models/segmentation/cell_segmentation/cellvit.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msegmentation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcell_segmentation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcellvit\u001b[39;00m \u001b[39mimport\u001b[39;00m CellViT, CellViT256, CellViTStarDist\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CellViTStarDist' from 'models.segmentation.cell_segmentation.cellvit' (/homes/fhoerst/histo-projects/CellViT/models/segmentation/cell_segmentation/cellvit.py)"
     ]
    }
   ],
   "source": [
    "from models.segmentation.cell_segmentation.cellvit import CellViT, CellViT256, CellViTStarDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CellViT256StarDist' from 'models.segmentation.cell_segmentation.cellvit' (/homes/fhoerst/histo-projects/CellViT/models/segmentation/cell_segmentation/cellvit.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msegmentation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcell_segmentation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcellvit\u001b[39;00m \u001b[39mimport\u001b[39;00m CellViT256StarDist\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CellViT256StarDist' from 'models.segmentation.cell_segmentation.cellvit' (/homes/fhoerst/histo-projects/CellViT/models/segmentation/cell_segmentation/cellvit.py)"
     ]
    }
   ],
   "source": [
    "from models.segmentation.cell_segmentation.cellvit import CellViT256StarDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellViTStarDist(CellViT):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nuclei_classes: int,\n",
    "        num_tissue_classes: int,\n",
    "        embed_dim: int,\n",
    "        input_channels: int,\n",
    "        depth: int,\n",
    "        num_heads: int,\n",
    "        extract_layers: List,\n",
    "        nrays: int = 32,\n",
    "        mlp_ratio: float = 4,\n",
    "        qkv_bias: bool = True,\n",
    "        drop_rate: float = 0,\n",
    "        attn_drop_rate: float = 0,\n",
    "        drop_path_rate: float = 0,\n",
    "        regression_loss: bool = False,\n",
    "    ):\n",
    "        super(CellViT, self).__init__()\n",
    "        assert len(extract_layers) == 4, \"Please provide 4 layers for skip connections\"\n",
    "\n",
    "        self.patch_size = 16\n",
    "        self.num_tissue_classes = num_tissue_classes\n",
    "        self.num_nuclei_classes = num_nuclei_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_channels = input_channels\n",
    "        self.depth = depth\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.extract_layers = extract_layers\n",
    "        self.drop_rate = drop_rate\n",
    "        self.attn_drop_rate = attn_drop_rate\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "        self.nrays = nrays\n",
    "        self.prompt_embed_dim = 256\n",
    "\n",
    "        self.encoder = ViTCellViT(\n",
    "            patch_size=self.patch_size,\n",
    "            num_classes=self.num_tissue_classes,\n",
    "            embed_dim=self.embed_dim,\n",
    "            depth=self.depth,\n",
    "            num_heads=self.num_heads,\n",
    "            mlp_ratio=self.mlp_ratio,\n",
    "            qkv_bias=self.qkv_bias,\n",
    "            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "            extract_layers=self.extract_layers,\n",
    "            drop_rate=drop_rate,\n",
    "            attn_drop_rate=attn_drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "        )\n",
    "\n",
    "        if self.embed_dim < 512:\n",
    "            self.skip_dim_11 = 256\n",
    "            self.skip_dim_12 = 128\n",
    "            self.bottleneck_dim = 312\n",
    "        else:\n",
    "            self.skip_dim_11 = 512\n",
    "            self.skip_dim_12 = 256\n",
    "            self.bottleneck_dim = 512\n",
    "\n",
    "        # version with shared skip_connections\n",
    "        self.decoder0 = nn.Sequential(\n",
    "            Conv2DBlock(3, 32, 3, dropout=self.drop_rate),\n",
    "            Conv2DBlock(32, 64, 3, dropout=self.drop_rate),\n",
    "        )  # skip connection after positional encoding, shape should be H, W, 64\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            Deconv2DBlock(self.embed_dim, self.skip_dim_11, dropout=self.drop_rate),\n",
    "            Deconv2DBlock(self.skip_dim_11, self.skip_dim_12, dropout=self.drop_rate),\n",
    "            Deconv2DBlock(self.skip_dim_12, 128, dropout=self.drop_rate),\n",
    "        )  # skip connection 1\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            Deconv2DBlock(self.embed_dim, self.skip_dim_11, dropout=self.drop_rate),\n",
    "            Deconv2DBlock(self.skip_dim_11, 256, dropout=self.drop_rate),\n",
    "        )  # skip connection 2\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            Deconv2DBlock(self.embed_dim, self.bottleneck_dim, dropout=self.drop_rate)\n",
    "        )  # skip connection 3\n",
    "        self.branches_output = {\n",
    "            \"stardist\": self.nrays,\n",
    "            \"dist\": 1,\n",
    "            \"nuclei_type_maps\": self.num_nuclei_classes,\n",
    "        }\n",
    "        self.stardist_decoder = self.create_upsampling_branch(\n",
    "            self.branches_output[\"stardist\"]\n",
    "        )\n",
    "        self.dist_decoder = self.create_upsampling_branch(\n",
    "            self.branches_output[\"dist\"]\n",
    "        )\n",
    "        self.nuclei_type_maps_decoder = self.create_upsampling_branch(\n",
    "            self.num_nuclei_classes\n",
    "        )\n",
    "        self.classifier_head = (\n",
    "            nn.Linear(self.prompt_embed_dim, num_tissue_classes)\n",
    "            if num_tissue_classes > 0\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, retrieve_tokens: bool = False) -> dict:\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Images in BCHW style\n",
    "            retrieve_tokens (bool, optional): If tokens of ViT should be returned as well. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            dict: Output for all branches:\n",
    "                * tissue_types: Raw tissue type prediction. Shape: (batch_size, num_tissue_classes)\n",
    "                * nuclei_binary_map: Raw binary cell segmentation predictions. Shape: (batch_size, 2, H, W)\n",
    "                * hv_map: Binary HV Map predictions. Shape: (batch_size, 2, H, W)\n",
    "                * nuclei_type_map: Raw binary nuclei type preditcions. Shape: (batch_size, num_nuclei_classes, H, W)\n",
    "                * (optinal) tokens\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            x.shape[-2] % self.patch_size == 0\n",
    "        ), \"Img must have a shape of that is divisible by patch_size (token_size)\"\n",
    "        assert (\n",
    "            x.shape[-1] % self.patch_size == 0\n",
    "        ), \"Img must have a shape of that is divisible by patch_size (token_size)\"\n",
    "\n",
    "        out_dict = {}\n",
    "\n",
    "        classifier_logits, _, z = self.encoder(x)\n",
    "        out_dict[\"tissue_types\"] = classifier_logits\n",
    "\n",
    "        z0, z1, z2, z3, z4 = x, *z\n",
    "\n",
    "        # performing reshape for the convolutional layers and upsampling (restore spatial dimension)\n",
    "        patch_dim = [int(d / self.patch_size) for d in [x.shape[-2], x.shape[-1]]]\n",
    "        z4 = z4[:, 1:, :].transpose(-1, -2).view(-1, self.embed_dim, *patch_dim)\n",
    "        z3 = z3[:, 1:, :].transpose(-1, -2).view(-1, self.embed_dim, *patch_dim)\n",
    "        z2 = z2[:, 1:, :].transpose(-1, -2).view(-1, self.embed_dim, *patch_dim)\n",
    "        z1 = z1[:, 1:, :].transpose(-1, -2).view(-1, self.embed_dim, *patch_dim)\n",
    "\n",
    "        out_dict[\"nuclei_binary_map\"] = self._forward_upsample(\n",
    "            z0, z1, z2, z3, z4, self.stardist_decoder\n",
    "        )\n",
    "        out_dict[\"hv_map\"] = self._forward_upsample(\n",
    "            z0, z1, z2, z3, z4, self.dist_decoder\n",
    "        )\n",
    "        out_dict[\"nuclei_type_map\"] = self._forward_upsample(\n",
    "            z0, z1, z2, z3, z4, self.nuclei_type_maps_decoder\n",
    "        )\n",
    "        if retrieve_tokens:\n",
    "            out_dict[\"tokens\"] = z4\n",
    "\n",
    "        return out_dict\n",
    "    \n",
    " \n",
    "class CellViT256StarDist(CellViTStarDist, CellViT256):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model256_path: Union[Path, str],\n",
    "        num_nuclei_classes: int,\n",
    "        num_tissue_classes: int,\n",
    "        nrays: int = 32,\n",
    "        drop_rate: float = 0,\n",
    "        attn_drop_rate: float = 0,\n",
    "        drop_path_rate: float = 0,\n",
    "        regression_loss: bool = False,  # to use regressive loss for predicting vector components\n",
    "    ):\n",
    "        self.patch_size = 16\n",
    "        self.embed_dim = 384\n",
    "        self.depth = 12\n",
    "        self.num_heads = 6\n",
    "        self.mlp_ratio = 4\n",
    "        self.qkv_bias = True\n",
    "        self.extract_layers = [3, 6, 9, 12]\n",
    "        self.input_channels = 3  # RGB\n",
    "        self.num_tissue_classes = num_tissue_classes\n",
    "        self.num_nuclei_classes = num_nuclei_classes\n",
    "        self.nrays = nrays\n",
    "        \n",
    "        super().__init__(\n",
    "            num_nuclei_classes=num_nuclei_classes,\n",
    "            num_tissue_classes=num_tissue_classes,\n",
    "            embed_dim=self.embed_dim,\n",
    "            input_channels=self.input_channels,\n",
    "            depth=self.depth,\n",
    "            num_heads=self.num_heads,\n",
    "            extract_layers=self.extract_layers,\n",
    "            mlp_ratio=self.mlp_ratio,\n",
    "            qkv_bias=self.qkv_bias,\n",
    "            drop_rate=drop_rate,\n",
    "            attn_drop_rate=attn_drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            regression_loss=regression_loss,\n",
    "            nrays=self.nrays\n",
    "        )\n",
    "\n",
    "        self.model256_path = model256_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ViTStarDist(\n",
    "#     model_path=\"/homes/fhoerst/histo-projects/CellViT/models/pretrained/SAM/sam_vit_h.pth\",\n",
    "#     num_nuclei_classes=6,\n",
    "#     num_tissue_classes=19,\n",
    "#     vit_structure=\"SAM-H\",\n",
    "#     drop_rate=0\n",
    "# )\n",
    "\n",
    "\n",
    "model = CellViT256StarDist(\n",
    "    model256_path=\"/homes/fhoerst/histo-projects/CellViT/models/pretrained/ViT-256/vit256_small_dino.pth\",\n",
    "    num_nuclei_classes=6,\n",
    "    num_tissue_classes=19\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "CellViT256StarDist                            [1, 6, 256, 256]          4,883\n",
       "├─ViTCellViT: 1-1                             [1, 19]                   76,032\n",
       "│    └─PatchEmbed: 2-1                        [1, 256, 384]             --\n",
       "│    │    └─Conv2d: 3-1                       [1, 384, 16, 16]          295,296\n",
       "│    └─Dropout: 2-2                           [1, 257, 384]             --\n",
       "│    └─ModuleList: 2-3                        --                        --\n",
       "│    │    └─Block: 3-2                        [1, 257, 384]             1,774,464\n",
       "│    │    └─Block: 3-3                        [1, 257, 384]             1,774,464\n",
       "│    │    └─Block: 3-4                        [1, 257, 384]             1,774,464\n",
       "│    │    └─Block: 3-5                        [1, 257, 384]             1,774,464\n",
       "│    │    └─Block: 3-6                        [1, 257, 384]             1,774,464\n",
       "│    │    └─Block: 3-7                        [1, 257, 384]             1,774,464\n",
       "│    │    └─Block: 3-8                        [1, 257, 384]             1,774,464\n",
       "│    │    └─Block: 3-9                        [1, 257, 384]             1,774,464\n",
       "│    │    └─Block: 3-10                       [1, 257, 384]             1,774,464\n",
       "│    │    └─Block: 3-11                       [1, 257, 384]             1,774,464\n",
       "│    │    └─Block: 3-12                       [1, 257, 384]             1,774,464\n",
       "│    │    └─Block: 3-13                       [1, 257, 384]             1,774,464\n",
       "│    └─LayerNorm: 2-4                         [1, 257, 384]             768\n",
       "│    └─Linear: 2-5                            [1, 19]                   7,315\n",
       "├─Sequential: 1-10                            --                        (recursive)\n",
       "│    └─ConvTranspose2d: 2-6                   [1, 312, 32, 32]          479,544\n",
       "├─Sequential: 1-3                             [1, 312, 32, 32]          --\n",
       "│    └─Deconv2DBlock: 2-7                     [1, 312, 32, 32]          --\n",
       "│    │    └─Sequential: 3-14                  [1, 312, 32, 32]          1,356,576\n",
       "├─Sequential: 1-10                            --                        (recursive)\n",
       "│    └─Sequential: 2-8                        [1, 256, 64, 64]          --\n",
       "│    │    └─Conv2DBlock: 3-15                 [1, 312, 32, 32]          1,753,128\n",
       "│    │    └─Conv2DBlock: 3-16                 [1, 312, 32, 32]          877,032\n",
       "│    │    └─Conv2DBlock: 3-17                 [1, 312, 32, 32]          877,032\n",
       "│    │    └─ConvTranspose2d: 3-18             [1, 256, 64, 64]          319,744\n",
       "├─Sequential: 1-5                             [1, 256, 64, 64]          --\n",
       "│    └─Deconv2DBlock: 2-9                     [1, 256, 32, 32]          --\n",
       "│    │    └─Sequential: 3-19                  [1, 256, 32, 32]          984,064\n",
       "│    └─Deconv2DBlock: 2-10                    [1, 256, 64, 64]          --\n",
       "│    │    └─Sequential: 3-20                  [1, 256, 64, 64]          852,992\n",
       "├─Sequential: 1-10                            --                        (recursive)\n",
       "│    └─Sequential: 2-11                       [1, 128, 128, 128]        --\n",
       "│    │    └─Conv2DBlock: 3-21                 [1, 256, 64, 64]          1,180,416\n",
       "│    │    └─Conv2DBlock: 3-22                 [1, 256, 64, 64]          590,592\n",
       "│    │    └─ConvTranspose2d: 3-23             [1, 128, 128, 128]        131,200\n",
       "├─Sequential: 1-7                             [1, 128, 128, 128]        --\n",
       "│    └─Deconv2DBlock: 2-12                    [1, 256, 32, 32]          --\n",
       "│    │    └─Sequential: 3-24                  [1, 256, 32, 32]          984,064\n",
       "│    └─Deconv2DBlock: 2-13                    [1, 128, 64, 64]          --\n",
       "│    │    └─Sequential: 3-25                  [1, 128, 64, 64]          279,040\n",
       "│    └─Deconv2DBlock: 2-14                    [1, 128, 128, 128]        --\n",
       "│    │    └─Sequential: 3-26                  [1, 128, 128, 128]        213,504\n",
       "├─Sequential: 1-10                            --                        (recursive)\n",
       "│    └─Sequential: 2-15                       [1, 64, 256, 256]         --\n",
       "│    │    └─Conv2DBlock: 3-27                 [1, 128, 128, 128]        295,296\n",
       "│    │    └─Conv2DBlock: 3-28                 [1, 128, 128, 128]        147,840\n",
       "│    │    └─ConvTranspose2d: 3-29             [1, 64, 256, 256]         32,832\n",
       "├─Sequential: 1-9                             [1, 64, 256, 256]         --\n",
       "│    └─Conv2DBlock: 2-16                      [1, 32, 256, 256]         --\n",
       "│    │    └─Sequential: 3-30                  [1, 32, 256, 256]         960\n",
       "│    └─Conv2DBlock: 2-17                      [1, 64, 256, 256]         --\n",
       "│    │    └─Sequential: 3-31                  [1, 64, 256, 256]         18,624\n",
       "├─Sequential: 1-10                            --                        (recursive)\n",
       "│    └─Sequential: 2-18                       [1, 32, 256, 256]         --\n",
       "│    │    └─Conv2DBlock: 3-32                 [1, 64, 256, 256]         73,920\n",
       "│    │    └─Conv2DBlock: 3-33                 [1, 64, 256, 256]         37,056\n",
       "│    │    └─Conv2d: 3-34                      [1, 32, 256, 256]         2,080\n",
       "├─Sequential: 1-19                            --                        (recursive)\n",
       "│    └─ConvTranspose2d: 2-19                  [1, 312, 32, 32]          479,544\n",
       "├─Sequential: 1-12                            [1, 312, 32, 32]          (recursive)\n",
       "│    └─Deconv2DBlock: 2-20                    [1, 312, 32, 32]          (recursive)\n",
       "│    │    └─Sequential: 3-35                  [1, 312, 32, 32]          (recursive)\n",
       "├─Sequential: 1-19                            --                        (recursive)\n",
       "│    └─Sequential: 2-21                       [1, 256, 64, 64]          --\n",
       "│    │    └─Conv2DBlock: 3-36                 [1, 312, 32, 32]          1,753,128\n",
       "│    │    └─Conv2DBlock: 3-37                 [1, 312, 32, 32]          877,032\n",
       "│    │    └─Conv2DBlock: 3-38                 [1, 312, 32, 32]          877,032\n",
       "│    │    └─ConvTranspose2d: 3-39             [1, 256, 64, 64]          319,744\n",
       "├─Sequential: 1-14                            [1, 256, 64, 64]          (recursive)\n",
       "│    └─Deconv2DBlock: 2-22                    [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Sequential: 3-40                  [1, 256, 32, 32]          (recursive)\n",
       "│    └─Deconv2DBlock: 2-23                    [1, 256, 64, 64]          (recursive)\n",
       "│    │    └─Sequential: 3-41                  [1, 256, 64, 64]          (recursive)\n",
       "├─Sequential: 1-19                            --                        (recursive)\n",
       "│    └─Sequential: 2-24                       [1, 128, 128, 128]        --\n",
       "│    │    └─Conv2DBlock: 3-42                 [1, 256, 64, 64]          1,180,416\n",
       "│    │    └─Conv2DBlock: 3-43                 [1, 256, 64, 64]          590,592\n",
       "│    │    └─ConvTranspose2d: 3-44             [1, 128, 128, 128]        131,200\n",
       "├─Sequential: 1-16                            [1, 128, 128, 128]        (recursive)\n",
       "│    └─Deconv2DBlock: 2-25                    [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Sequential: 3-45                  [1, 256, 32, 32]          (recursive)\n",
       "│    └─Deconv2DBlock: 2-26                    [1, 128, 64, 64]          (recursive)\n",
       "│    │    └─Sequential: 3-46                  [1, 128, 64, 64]          (recursive)\n",
       "│    └─Deconv2DBlock: 2-27                    [1, 128, 128, 128]        (recursive)\n",
       "│    │    └─Sequential: 3-47                  [1, 128, 128, 128]        (recursive)\n",
       "├─Sequential: 1-19                            --                        (recursive)\n",
       "│    └─Sequential: 2-28                       [1, 64, 256, 256]         --\n",
       "│    │    └─Conv2DBlock: 3-48                 [1, 128, 128, 128]        295,296\n",
       "│    │    └─Conv2DBlock: 3-49                 [1, 128, 128, 128]        147,840\n",
       "│    │    └─ConvTranspose2d: 3-50             [1, 64, 256, 256]         32,832\n",
       "├─Sequential: 1-18                            [1, 64, 256, 256]         (recursive)\n",
       "│    └─Conv2DBlock: 2-29                      [1, 32, 256, 256]         (recursive)\n",
       "│    │    └─Sequential: 3-51                  [1, 32, 256, 256]         (recursive)\n",
       "│    └─Conv2DBlock: 2-30                      [1, 64, 256, 256]         (recursive)\n",
       "│    │    └─Sequential: 3-52                  [1, 64, 256, 256]         (recursive)\n",
       "├─Sequential: 1-19                            --                        (recursive)\n",
       "│    └─Sequential: 2-31                       [1, 1, 256, 256]          --\n",
       "│    │    └─Conv2DBlock: 3-53                 [1, 64, 256, 256]         73,920\n",
       "│    │    └─Conv2DBlock: 3-54                 [1, 64, 256, 256]         37,056\n",
       "│    │    └─Conv2d: 3-55                      [1, 1, 256, 256]          65\n",
       "├─Sequential: 1-28                            --                        (recursive)\n",
       "│    └─ConvTranspose2d: 2-32                  [1, 312, 32, 32]          479,544\n",
       "├─Sequential: 1-21                            [1, 312, 32, 32]          (recursive)\n",
       "│    └─Deconv2DBlock: 2-33                    [1, 312, 32, 32]          (recursive)\n",
       "│    │    └─Sequential: 3-56                  [1, 312, 32, 32]          (recursive)\n",
       "├─Sequential: 1-28                            --                        (recursive)\n",
       "│    └─Sequential: 2-34                       [1, 256, 64, 64]          --\n",
       "│    │    └─Conv2DBlock: 3-57                 [1, 312, 32, 32]          1,753,128\n",
       "│    │    └─Conv2DBlock: 3-58                 [1, 312, 32, 32]          877,032\n",
       "│    │    └─Conv2DBlock: 3-59                 [1, 312, 32, 32]          877,032\n",
       "│    │    └─ConvTranspose2d: 3-60             [1, 256, 64, 64]          319,744\n",
       "├─Sequential: 1-23                            [1, 256, 64, 64]          (recursive)\n",
       "│    └─Deconv2DBlock: 2-35                    [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Sequential: 3-61                  [1, 256, 32, 32]          (recursive)\n",
       "│    └─Deconv2DBlock: 2-36                    [1, 256, 64, 64]          (recursive)\n",
       "│    │    └─Sequential: 3-62                  [1, 256, 64, 64]          (recursive)\n",
       "├─Sequential: 1-28                            --                        (recursive)\n",
       "│    └─Sequential: 2-37                       [1, 128, 128, 128]        --\n",
       "│    │    └─Conv2DBlock: 3-63                 [1, 256, 64, 64]          1,180,416\n",
       "│    │    └─Conv2DBlock: 3-64                 [1, 256, 64, 64]          590,592\n",
       "│    │    └─ConvTranspose2d: 3-65             [1, 128, 128, 128]        131,200\n",
       "├─Sequential: 1-25                            [1, 128, 128, 128]        (recursive)\n",
       "│    └─Deconv2DBlock: 2-38                    [1, 256, 32, 32]          (recursive)\n",
       "│    │    └─Sequential: 3-66                  [1, 256, 32, 32]          (recursive)\n",
       "│    └─Deconv2DBlock: 2-39                    [1, 128, 64, 64]          (recursive)\n",
       "│    │    └─Sequential: 3-67                  [1, 128, 64, 64]          (recursive)\n",
       "│    └─Deconv2DBlock: 2-40                    [1, 128, 128, 128]        (recursive)\n",
       "│    │    └─Sequential: 3-68                  [1, 128, 128, 128]        (recursive)\n",
       "├─Sequential: 1-28                            --                        (recursive)\n",
       "│    └─Sequential: 2-41                       [1, 64, 256, 256]         --\n",
       "│    │    └─Conv2DBlock: 3-69                 [1, 128, 128, 128]        295,296\n",
       "│    │    └─Conv2DBlock: 3-70                 [1, 128, 128, 128]        147,840\n",
       "│    │    └─ConvTranspose2d: 3-71             [1, 64, 256, 256]         32,832\n",
       "├─Sequential: 1-27                            [1, 64, 256, 256]         (recursive)\n",
       "│    └─Conv2DBlock: 2-42                      [1, 32, 256, 256]         (recursive)\n",
       "│    │    └─Sequential: 3-72                  [1, 32, 256, 256]         (recursive)\n",
       "│    └─Conv2DBlock: 2-43                      [1, 64, 256, 256]         (recursive)\n",
       "│    │    └─Sequential: 3-73                  [1, 64, 256, 256]         (recursive)\n",
       "├─Sequential: 1-28                            --                        (recursive)\n",
       "│    └─Sequential: 2-44                       [1, 6, 256, 256]          --\n",
       "│    │    └─Conv2DBlock: 3-74                 [1, 64, 256, 256]         73,920\n",
       "│    │    └─Conv2DBlock: 3-75                 [1, 64, 256, 256]         37,056\n",
       "│    │    └─Conv2d: 3-76                      [1, 6, 256, 256]          390\n",
       "===============================================================================================\n",
       "Total params: 46,757,117\n",
       "Trainable params: 46,757,117\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 133.01\n",
       "===============================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 1687.70\n",
       "Params size (MB): 186.70\n",
       "Estimated Total Size (MB): 1875.19\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(1, 3, 256, 256), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellvit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
